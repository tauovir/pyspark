{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df9117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window as W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecffc3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Luxsoft\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2e1e4",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1303ac",
   "metadata": {},
   "source": [
    "**Q1. What would be the answer of below query **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "936af2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| 10|\n",
      "+---+\n",
      "| 10|\n",
      "| 20|\n",
      "| 10|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Q1. What would be the answer of below query\n",
    "sql = \"\"\"\n",
    "select 10\n",
    "union select 10\n",
    "union select 20\n",
    "union  select 20\n",
    "union all select 10\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48283e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|  id| cd|\n",
      "+----+---+\n",
      "|  10|abc|\n",
      "|  20|abc|\n",
      "|NULL|abc|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(10,'abc'),(20,'abc'),(None,'abc')]\n",
    "schema = \"id int, cd string\"\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124661b3",
   "metadata": {},
   "source": [
    "**Q3. What would be the output of below query **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11740abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id| cd|\n",
      "+---+---+\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('T1')\n",
    "sql = \"\"\"\n",
    "select id, cd  from T1 where ID = Null;\n",
    "\"\"\"\n",
    "spark.sql(sql).show()\n",
    "\n",
    "##You cannot use = NULL or <> NULL because NULL is not equal or unequal to anything. NULL means unknown.\n",
    "## So use IS NULL or IS NOT NULL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71df6b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|  id| cd|\n",
      "+----+---+\n",
      "|NULL|abc|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "select id, cd  from T1 where ID is null;\n",
    "\"\"\"\n",
    "spark.sql(sql).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39cd10b",
   "metadata": {},
   "source": [
    "**Q4. What would be the output of below query **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2344b028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+------------------+-------+\n",
      "|count(1)|count(ID)|count(cd)|count(DISTINCT cd)|sum(ID)|\n",
      "+--------+---------+---------+------------------+-------+\n",
      "|       3|        2|        3|                 1|     30|\n",
      "+--------+---------+---------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "SELECT count(*),count(ID),count(cd),count(distinct cd),sum(ID) from T1\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0606184",
   "metadata": {},
   "source": [
    "**Q5. What would be the output of below query **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c02feb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|count(id)|\n",
      "+---------+\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SQL = \"\"\"\n",
    "SELECT count(id) from T1\n",
    "group by cd having count(id) = 1\n",
    "\"\"\"\n",
    "spark.sql(SQL).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5260704d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- notes: string (nullable = true)\n",
      " |-- sales_date: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = [(10,'mango','2024-01-01',100),\n",
    "(10,'orange','2024-01-02',120),\n",
    "(11,'jeans','2024-01-03',200),\n",
    "(11,'jeans','2024-01-03',250),\n",
    "(11,'T-shirt','2024-01-04',200),\n",
    "(12,'Banana','2024-01-04',50)]\n",
    "data_schema = \"id int, notes string,sales_date string,amount int\"\n",
    "dataframe = spark.createDataFrame(data = dataset, schema = data_schema)\n",
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48f70122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- notes: string (nullable = true)\n",
      " |-- sales_date: date (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n",
      "+---+-------+----------+------+\n",
      "| id|  notes|sales_date|amount|\n",
      "+---+-------+----------+------+\n",
      "| 10|  mango|2024-01-01|   100|\n",
      "| 10| orange|2024-01-02|   120|\n",
      "| 11|  jeans|2024-01-03|   200|\n",
      "| 11|  jeans|2024-01-03|   250|\n",
      "| 11|T-shirt|2024-01-04|   200|\n",
      "| 12| Banana|2024-01-04|    50|\n",
      "+---+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert sales_date data types\n",
    "from pyspark.sql.types import DateType\n",
    "dataframe2 = dataframe.withColumn(\"sales_date\",dataframe['sales_date'].cast(DateType()))\n",
    "# dataframe3 = dataframe.withColumn(\"sales_date\",F.to_date('sales_date','yyyy-mm-dd'))\n",
    "dataframe2.printSchema()\n",
    "dataframe2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ec46c",
   "metadata": {},
   "source": [
    "**Q6--- fetch the max sale amount for each date ID wise, also get sum of sales amount id wise ---**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5cad2836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+---------------+\n",
      "| id|  notes|sales_date|amount|commulative_sum|\n",
      "+---+-------+----------+------+---------------+\n",
      "| 10|  mango|2024-01-01|   100|            100|\n",
      "| 10| orange|2024-01-02|   120|            220|\n",
      "| 11|  jeans|2024-01-03|   250|            650|\n",
      "| 11|T-shirt|2024-01-04|   200|            400|\n",
      "| 12| Banana|2024-01-04|    50|             50|\n",
      "+---+-------+----------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe2.createOrReplaceTempView('sales')\n",
    "sql = \"\"\"\n",
    "WITH data_query as \n",
    "(\n",
    "SELECT id,notes,sales_date,amount,\n",
    "DENSE_RANK() OVER (PARTITION BY id,sales_date order by amount desc) as rnk,\n",
    "sum(amount) OVER (PARTITION BY id order by amount asc) as commulative_sum\n",
    "from sales\n",
    ")\n",
    "select id,notes,sales_date,amount,commulative_sum from data_query\n",
    "where rnk = 1\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b035370b",
   "metadata": {},
   "source": [
    "**Q:7:Produce Below output **\n",
    "```\n",
    "id\t\tnote\n",
    "10\t\"mango,orange\"\n",
    "11\t\"jeans,T-shirt\"\n",
    "12\t\"Banana\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73e67b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_sql = \"\"\"\n",
    "SELECT id,STRING_AGG(DISTINCT notes,',') as note from sales group by id\n",
    "\"\"\"\n",
    "snow_sql = \"SELECT id,listagg(DISTINCT notes,',') as note from sales group by id\"\n",
    "# spark.sql(postgres_sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c095a22e",
   "metadata": {},
   "source": [
    "**Q:8 => Number of records with Right join, full outer join between two table **<br>\n",
    "**See reference for similar question**:<br>\n",
    "https://github.com/tauovir/pyspark/blob/master/src_notebok/vpropel/spark/Spark_Joins.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b69a6eb",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "28a0fd03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1: which is fast dictionary or tuple in term of data access\n",
    "#2: threading vs multi-processing\n",
    "#3: What would be output for l1,l2\n",
    "#4: how do you get next value from l2\n",
    "#\n",
    "l1 = [ele for ele in range(1,5)]\n",
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fbe87aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x0000026922DF9CB0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2 = (ele for ele in range(1,5))\n",
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1643d3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2.__next__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b87f34d",
   "metadata": {},
   "source": [
    "### Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6e6c24d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every day you are getting large amount data file but it could have 5-10% new or updated data, how acces those data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e209a84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
