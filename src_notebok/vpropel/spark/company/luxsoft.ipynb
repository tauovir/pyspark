{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a7475d-1a08-43db-a039-608df4fbf3b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window as W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d34b4da-8820-4bd3-9bce-e0979e9721fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType,StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aafc8bd9-d9bc-4ea3-ab67-af84025c5441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Luxsoft\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92ba00b6-9719-419d-b6a9-7f58e209050d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5344e39-13a9-4d14-8f46-af00bcf43632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q1. What would be the answer of below query **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d10dd83e-0a60-4736-a3e0-ab83bd6bcd6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Q1. What would be the answer of below query\n",
    "sql = \"\"\"\n",
    "select 10\n",
    "union select 10\n",
    "union select 20\n",
    "union  select 20\n",
    "union all select 10\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb567756-4916-4daa-a395-401e27b3fdb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(10,'abc'),(20,'abc'),(None,'abc')]\n",
    "schema = \"id int, cd string\"\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f90315bd-e410-49fc-9077-aa9b974177fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q3. What would be the output of below query **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8497ff35-3ab5-4394-83d4-55d8ed356c3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('T1')\n",
    "sql = \"\"\"\n",
    "select id, cd  from T1 where ID = Null;\n",
    "\"\"\"\n",
    "spark.sql(sql).show()\n",
    "\n",
    "##You cannot use = NULL or <> NULL because NULL is not equal or unequal to anything. NULL means unknown.\n",
    "## So use IS NULL or IS NOT NULL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dd4419c-c6d7-4003-92dc-6fe480b2bc2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(F.col('id').isNull()).show()\n",
    "# df.filter(\"id is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05edc4f9-25be-445f-8c56-10beaa6b5a31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "select id, cd  from T1 where ID is null;\n",
    "\"\"\"\n",
    "spark.sql(sql).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be65e8f6-3c37-40c4-accc-13208362c98b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df.filter(df.id.isNotNull()).show()\n",
    "df.filter(df.id.isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe90feef-bd3d-47dd-8585-516a68905df9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q4. What would be the output of below query **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "231f0d74-caca-4c30-8ab4-5c180f4fae04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT count(*),count(ID),count(cd),count(distinct cd),sum(ID) from T1\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf900769-9d51-4a15-a5a0-321fc748034b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.agg(F.count('*').alias('cnt'),\n",
    "       F.count('id').alias('id_cnt'),\n",
    "        F.count('id').alias('id_cnt'),\n",
    "        F.countDistinct('cd').alias('dcd_cnt'),\n",
    "        F.sum('id').alias('sum_cnt'),\n",
    "      \n",
    "      ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78813085-5649-4413-9740-b9cca8d15e4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q5. What would be the output of below query **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27e4958e-bb7a-4f84-9078-fbcf54962880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SQL = \"\"\"\n",
    "SELECT count(id) from T1\n",
    "group by cd having count(id) = 1\n",
    "\"\"\"\n",
    "spark.sql(SQL).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0216ce2-b00b-457c-a5c0-271774c7a89a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"cd\").agg(F.count('id').alias(\"cnt_id\")).filter(F.col('cnt_id') == 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d67e1b5-747a-40a4-9a91-e34ce6c933b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset = [(10,'mango','2024-01-01',100),\n",
    "(10,'orange','2024-01-02',120),\n",
    "(11,'jeans','2024-01-03',200),\n",
    "(11,'jeans','2024-01-03',250),\n",
    "(11,'T-shirt','2024-01-04',200),\n",
    "(12,'Banana','2024-01-04',50)]\n",
    "data_schema = \"id int, notes string,sales_date string,amount int\"\n",
    "dataframe = spark.createDataFrame(data = dataset, schema = data_schema)\n",
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c27372ee-59d0-4a99-9498-61bf2370addd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert sales_date data types\n",
    "from pyspark.sql.types import DateType\n",
    "dataframe2 = dataframe.withColumn(\"sales_date\",dataframe['sales_date'].cast(DateType()))\n",
    "# dataframe3 = dataframe.withColumn(\"sales_date\",F.to_date('sales_date','yyyy-mm-dd'))\n",
    "dataframe2.printSchema()\n",
    "dataframe2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "567eb418-dae0-4689-ac33-b96300608539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q6--- fetch the max sale amount for each date ID wise, also get sum of sales amount id wise ---**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caa4e712-7c39-4be1-ba0f-4e788fd1cf53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# IN window function sum,if we do not use orer by then it will summ all,if order there then commulative sum perfomed\n",
    "\n",
    "dataframe2.createOrReplaceTempView('sales')\n",
    "sql = \"\"\"\n",
    "WITH data_query as \n",
    "(\n",
    "SELECT id,notes,sales_date,amount,\n",
    "DENSE_RANK() OVER (PARTITION BY id,sales_date order by amount desc) as rnk,\n",
    "sum(amount) OVER (PARTITION BY id order by amount asc) as commulative_sum\n",
    "from sales\n",
    ")\n",
    "select id,notes,sales_date,amount,commulative_sum from data_query\n",
    "where rnk = 1\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35290feb-67bf-42b0-8dbe-bf518d4ec6aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q:7:Produce Below output **\n",
    "```\n",
    "id\t\tnote\n",
    "10\t\"mango,orange\"\n",
    "11\t\"jeans,T-shirt\"\n",
    "12\t\"Banana\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba547c5c-c610-4943-866b-5b70befad4f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "postgres_sql = \"\"\"\n",
    "SELECT id,STRING_AGG(DISTINCT notes,',') as note from sales group by id\n",
    "\"\"\"\n",
    "snow_sql = \"SELECT id,listagg(DISTINCT notes,',') as note from sales group by id\"\n",
    "# spark.sql(postgres_sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9444a7f7-f569-4e73-80b5-9c7eb107a407",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataframe2.groupBy('id').agg(F.collect_set('notes').alias('note')).\\\n",
    "withColumn('note',F.concat_ws(',',F.col('note'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6f39162-2c02-417c-aa3e-8366d3b883f0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "With array_join"
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "dataframe2.groupBy(\"id\").agg(\n",
    "    F.collect_set(\"notes\").alias(\"notes\")\n",
    ").withColumn(\"array_compact\",F.array_join(\"notes\",\",\"))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "834a2806-5503-49ee-aecf-9aac8838b2f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@F.udf(StringType())\n",
    "def concate_str(arr):\n",
    "    return ','.join(arr)\n",
    "\n",
    "dataframe2.groupBy('id').agg(F.collect_set('notes').alias('note')).withColumn('note',concate_str(F.col('note'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f0dae1d-b49e-40f3-a695-2e6cc2242f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Using Data Frame\n",
    "#### Method-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed637545-ac3e-47af-81fb-2216fe2369f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataframe2.select(\"id\",\"notes\",\"sales_date\",\"amount\").\\\n",
    "withColumn(\"DenseRank\",F.dense_rank().over(W.partitionBy(\"id\").orderBy(F.col(\"amount\").desc()))).\\\n",
    "withColumn(\"Sum\",F.sum('amount').over(W.partitionBy(\"id\").orderBy(F.col(\"amount\").asc()))).\\\n",
    "filter(F.col(\"DenseRank\") ==1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77e0ec4b-f776-4f84-8cec-29a9e49061e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Method-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68975609-df95-48c3-8039-346c5d913e66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "agg_data = dataframe2.select(\"id\",\"notes\",\"sales_date\",\"amount\").groupBy(\"id\").agg(\n",
    "    F.sum(\"amount\").alias(\"sum_amount\"),\n",
    "    F.max(\"amount\").alias(\"max_amont\"),\n",
    ")\n",
    "dataframe2.join(agg_data,(dataframe2.id == agg_data.id) & (dataframe2.amount == agg_data.max_amont),\n",
    "                how=\"inner\" ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b8a52af-91d7-4f1c-8d58-7e6fa8038182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Collect List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36d671ea-23e4-4fcc-b91d-69f5e020a981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataframe2.groupBy('id').agg(F.collect_list(\"notes\").alias(\"notes\")).withColumn(\"_str\",F.concat_ws(\",\", F.col(\"notes\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1ff2856-0d0f-4711-959c-39ec4de78870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q:8 => Number of records with Right join, full outer join between two table **<br>\n",
    "**See reference for similar question**:<br>\n",
    "https://github.com/tauovir/pyspark/blob/master/src_notebok/vpropel/spark/Spark_Joins.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "718343c3-9e02-4511-bf6d-16ebaff801ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04fcdcee-ca58-4a51-8c39-ce5db9c696dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1: which is fast dictionary or tuple in term of data access\n",
    "#2: threading vs multi-processing\n",
    "#3: What would be output for l1,l2\n",
    "#4: how do you get next value from l2\n",
    "#\n",
    "l1 = [ele for ele in range(1,5)]\n",
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e341a791-ffd5-4cd3-bb72-d5fcbf2fea46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "l2 = (ele for ele in range(1,5))\n",
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d08cb124-e100-4126-87dd-350e9576b4ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "l2.__next__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20122384-6780-44bc-adb3-ca6636d0933f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c5a84c1-cf36-4cb8-81bb-ae4caf91eb38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Every day you are getting large amount data file but it could have 5-10% new or updated data, how acces those data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23ce3d71-7b44-49a7-ad82-cce86b35b7c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "luxsoft",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
