{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df9117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window as W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf821b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType,StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecffc3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Luxsoft\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2e1e4",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1303ac",
   "metadata": {},
   "source": [
    "**Q1. What would be the answer of below query **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "936af2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| 10|\n",
      "+---+\n",
      "| 10|\n",
      "| 20|\n",
      "| 10|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Q1. What would be the answer of below query\n",
    "sql = \"\"\"\n",
    "select 10\n",
    "union select 10\n",
    "union select 20\n",
    "union  select 20\n",
    "union all select 10\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48283e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|  id| cd|\n",
      "+----+---+\n",
      "|  10|abc|\n",
      "|  20|abc|\n",
      "|NULL|abc|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(10,'abc'),(20,'abc'),(None,'abc')]\n",
    "schema = \"id int, cd string\"\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124661b3",
   "metadata": {},
   "source": [
    "**Q3. What would be the output of below query **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11740abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id| cd|\n",
      "+---+---+\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('T1')\n",
    "sql = \"\"\"\n",
    "select id, cd  from T1 where ID = Null;\n",
    "\"\"\"\n",
    "spark.sql(sql).show()\n",
    "\n",
    "##You cannot use = NULL or <> NULL because NULL is not equal or unequal to anything. NULL means unknown.\n",
    "## So use IS NULL or IS NOT NULL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a11ce907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|  id| cd|\n",
      "+----+---+\n",
      "|NULL|abc|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(F.col('id').isNull()).show()\n",
    "# df.filter(\"id is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71df6b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|  id| cd|\n",
      "+----+---+\n",
      "|NULL|abc|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "select id, cd  from T1 where ID is null;\n",
    "\"\"\"\n",
    "spark.sql(sql).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49d632ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|  id| cd|\n",
      "+----+---+\n",
      "|NULL|abc|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.filter(df.id.isNotNull()).show()\n",
    "df.filter(df.id.isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39cd10b",
   "metadata": {},
   "source": [
    "**Q4. What would be the output of below query **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2344b028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+------------------+-------+\n",
      "|count(1)|count(ID)|count(cd)|count(DISTINCT cd)|sum(ID)|\n",
      "+--------+---------+---------+------------------+-------+\n",
      "|       3|        2|        3|                 1|     30|\n",
      "+--------+---------+---------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "SELECT count(*),count(ID),count(cd),count(distinct cd),sum(ID) from T1\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41e26ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+-------+\n",
      "|cnt|id_cnt|id_cnt|dcd_cnt|sum_cnt|\n",
      "+---+------+------+-------+-------+\n",
      "|  3|     2|     2|      1|     30|\n",
      "+---+------+------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(F.count('*').alias('cnt'),\n",
    "       F.count('id').alias('id_cnt'),\n",
    "        F.count('id').alias('id_cnt'),\n",
    "        F.countDistinct('cd').alias('dcd_cnt'),\n",
    "        F.sum('id').alias('sum_cnt'),\n",
    "      \n",
    "      ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0606184",
   "metadata": {},
   "source": [
    "**Q5. What would be the output of below query **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c02feb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|count(id)|\n",
      "+---------+\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SQL = \"\"\"\n",
    "SELECT count(id) from T1\n",
    "group by cd having count(id) = 1\n",
    "\"\"\"\n",
    "spark.sql(SQL).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01f15cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| cd|cnt_id|\n",
      "+---+------+\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"cd\").agg(F.count('id').alias(\"cnt_id\")).filter(F.col('cnt_id') == 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5260704d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- notes: string (nullable = true)\n",
      " |-- sales_date: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = [(10,'mango','2024-01-01',100),\n",
    "(10,'orange','2024-01-02',120),\n",
    "(11,'jeans','2024-01-03',200),\n",
    "(11,'jeans','2024-01-03',250),\n",
    "(11,'T-shirt','2024-01-04',200),\n",
    "(12,'Banana','2024-01-04',50)]\n",
    "data_schema = \"id int, notes string,sales_date string,amount int\"\n",
    "dataframe = spark.createDataFrame(data = dataset, schema = data_schema)\n",
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48f70122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- notes: string (nullable = true)\n",
      " |-- sales_date: date (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n",
      "+---+-------+----------+------+\n",
      "| id|  notes|sales_date|amount|\n",
      "+---+-------+----------+------+\n",
      "| 10|  mango|2024-01-01|   100|\n",
      "| 10| orange|2024-01-02|   120|\n",
      "| 11|  jeans|2024-01-03|   200|\n",
      "| 11|  jeans|2024-01-03|   250|\n",
      "| 11|T-shirt|2024-01-04|   200|\n",
      "| 12| Banana|2024-01-04|    50|\n",
      "+---+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert sales_date data types\n",
    "from pyspark.sql.types import DateType\n",
    "dataframe2 = dataframe.withColumn(\"sales_date\",dataframe['sales_date'].cast(DateType()))\n",
    "# dataframe3 = dataframe.withColumn(\"sales_date\",F.to_date('sales_date','yyyy-mm-dd'))\n",
    "dataframe2.printSchema()\n",
    "dataframe2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ec46c",
   "metadata": {},
   "source": [
    "**Q6--- fetch the max sale amount for each date ID wise, also get sum of sales amount id wise ---**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cad2836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+---------------+\n",
      "| id|  notes|sales_date|amount|commulative_sum|\n",
      "+---+-------+----------+------+---------------+\n",
      "| 10|  mango|2024-01-01|   100|            100|\n",
      "| 10| orange|2024-01-02|   120|            220|\n",
      "| 11|  jeans|2024-01-03|   250|            650|\n",
      "| 11|T-shirt|2024-01-04|   200|            400|\n",
      "| 12| Banana|2024-01-04|    50|             50|\n",
      "+---+-------+----------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IN window function sum,if we do not use orer by then it will summ all,if order there then commulative sum perfomed\n",
    "\n",
    "dataframe2.createOrReplaceTempView('sales')\n",
    "sql = \"\"\"\n",
    "WITH data_query as \n",
    "(\n",
    "SELECT id,notes,sales_date,amount,\n",
    "DENSE_RANK() OVER (PARTITION BY id,sales_date order by amount desc) as rnk,\n",
    "sum(amount) OVER (PARTITION BY id order by amount asc) as commulative_sum\n",
    "from sales\n",
    ")\n",
    "select id,notes,sales_date,amount,commulative_sum from data_query\n",
    "where rnk = 1\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b035370b",
   "metadata": {},
   "source": [
    "**Q:7:Produce Below output **\n",
    "```\n",
    "id\t\tnote\n",
    "10\t\"mango,orange\"\n",
    "11\t\"jeans,T-shirt\"\n",
    "12\t\"Banana\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73e67b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_sql = \"\"\"\n",
    "SELECT id,STRING_AGG(DISTINCT notes,',') as note from sales group by id\n",
    "\"\"\"\n",
    "snow_sql = \"SELECT id,listagg(DISTINCT notes,',') as note from sales group by id\"\n",
    "# spark.sql(postgres_sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7dced58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "| id|         note|\n",
      "+---+-------------+\n",
      "| 10| mango,orange|\n",
      "| 11|jeans,T-shirt|\n",
      "| 12|       Banana|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe2.groupBy('id').agg(F.collect_set('notes').alias('note')).\\\n",
    "withColumn('note',F.concat_ws(',',F.col('note'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b11abffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "| id|         note|\n",
      "+---+-------------+\n",
      "| 10| mango,orange|\n",
      "| 11|jeans,T-shirt|\n",
      "| 12|       Banana|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@F.udf(StringType())\n",
    "def concate_str(arr):\n",
    "    return ','.join(arr)\n",
    "\n",
    "dataframe2.groupBy('id').agg(F.collect_set('notes').alias('note')).withColumn('note',concate_str(F.col('note'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb7df18",
   "metadata": {},
   "source": [
    "### Using Data Frame\n",
    "#### Method-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83851c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+------+---------+---+\n",
      "| id| notes|sales_date|amount|DenseRank|Sum|\n",
      "+---+------+----------+------+---------+---+\n",
      "| 10|orange|2024-01-02|   120|        1|220|\n",
      "| 11| jeans|2024-01-03|   250|        1|650|\n",
      "| 12|Banana|2024-01-04|    50|        1| 50|\n",
      "+---+------+----------+------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe2.select(\"id\",\"notes\",\"sales_date\",\"amount\").\\\n",
    "withColumn(\"DenseRank\",F.dense_rank().over(W.partitionBy(\"id\").orderBy(F.col(\"amount\").desc()))).\\\n",
    "withColumn(\"Sum\",F.sum('amount').over(W.partitionBy(\"id\").orderBy(F.col(\"amount\").asc()))).\\\n",
    "filter(F.col(\"DenseRank\") ==1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee86990",
   "metadata": {},
   "source": [
    "#### Method-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8530328c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+------+---+----------+---------+\n",
      "| id| notes|sales_date|amount| id|sum_amount|max_amont|\n",
      "+---+------+----------+------+---+----------+---------+\n",
      "| 10|orange|2024-01-02|   120| 10|       220|      120|\n",
      "| 11| jeans|2024-01-03|   250| 11|       650|      250|\n",
      "| 12|Banana|2024-01-04|    50| 12|        50|       50|\n",
      "+---+------+----------+------+---+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_data = dataframe2.select(\"id\",\"notes\",\"sales_date\",\"amount\").groupBy(\"id\").agg(\n",
    "    F.sum(\"amount\").alias(\"sum_amount\"),\n",
    "    F.max(\"amount\").alias(\"max_amont\"),\n",
    ")\n",
    "dataframe2.join(agg_data,(dataframe2.id == agg_data.id) & (dataframe2.amount == agg_data.max_amont),\n",
    "                how=\"inner\" ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e0c88d",
   "metadata": {},
   "source": [
    "### Collect List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ba0805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+\n",
      "| id|               notes|               _str|\n",
      "+---+--------------------+-------------------+\n",
      "| 10|     [mango, orange]|       mango,orange|\n",
      "| 11|[jeans, jeans, T-...|jeans,jeans,T-shirt|\n",
      "| 12|            [Banana]|             Banana|\n",
      "+---+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe2.groupBy('id').agg(F.collect_list(\"notes\").alias(\"notes\")).withColumn(\"_str\",F.concat_ws(\",\", F.col(\"notes\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c095a22e",
   "metadata": {},
   "source": [
    "**Q:8 => Number of records with Right join, full outer join between two table **<br>\n",
    "**See reference for similar question**:<br>\n",
    "https://github.com/tauovir/pyspark/blob/master/src_notebok/vpropel/spark/Spark_Joins.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b69a6eb",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28a0fd03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1: which is fast dictionary or tuple in term of data access\n",
    "#2: threading vs multi-processing\n",
    "#3: What would be output for l1,l2\n",
    "#4: how do you get next value from l2\n",
    "#\n",
    "l1 = [ele for ele in range(1,5)]\n",
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbe87aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x000001747DAE5620>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2 = (ele for ele in range(1,5))\n",
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1643d3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2.__next__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b87f34d",
   "metadata": {},
   "source": [
    "### Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e6c24d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every day you are getting large amount data file but it could have 5-10% new or updated data, how acces those data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e209a84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
