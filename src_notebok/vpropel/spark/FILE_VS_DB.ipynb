{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd8d418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as FS\n",
    "from pyspark.sql import Window as WN\n",
    "from pyspark.sql import types as TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48236533",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"snow\").master(\"local[2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "abbd9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"E:/workforce/spark/data/Bengaluru_House_Data.csv\"\n",
    "output_path = \"E:/workforce/spark/data/output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74b903f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(spark,filepath):\n",
    "    return spark.read.format(\"csv\").options(path=datafile,header=True,inferSchema = True,delimiter = \",\").load()\n",
    "\n",
    "def get_db_credentials():\n",
    "    db_cred = {\n",
    "    \"url\" :\"jdbc:postgresql://localhost:5432/db101\",\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"khan102030\"\n",
    "    }\n",
    "    return db_cred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ea15e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_file(spark,filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bcea4fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           area_type|count|\n",
      "+--------------------+-----+\n",
      "|      Built-up  Area| 2418|\n",
      "|Super built-up  Area| 8790|\n",
      "|          Plot  Area| 2025|\n",
      "|        Carpet  Area|   87|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataframe.show()\n",
    "# dataframe.count()\n",
    "# dataframe.rdd.getNumPartitions()\n",
    "dataframe.groupBy(\"area_type\").agg(FS.count(\"*\").alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51e0c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file and database and see result: patrition: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d75e6e",
   "metadata": {},
   "source": [
    "### Case-1: Partition-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b4dba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It create one file and partition is one\n",
    "dataframe.write.format(\"csv\").mode('overwrite').option(\"path\",output_path).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa3725bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It creates on table\n",
    "dataframe.select(\"*\").write.format(\"jdbc\").options(**db_cred).\\\n",
    "option(\"dbtable\", 'house102').mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394627db",
   "metadata": {},
   "source": [
    "### Case-2: Partition-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e90e304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe2 = dataframe.repartition(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "657afb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4494bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file and database and see result: patrition: 3\n",
    "# It create three file and partition is 3 and each executers write\n",
    "dataframe2.write.format(\"csv\").mode('overwrite').option(\"path\",output_path).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55d4a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # It creates a table\n",
    "dataframe2.select(\"*\").write.format(\"jdbc\").options(**db_cred).\\\n",
    "option(\"dbtable\", 'house102').mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743ade9a",
   "metadata": {},
   "source": [
    "### Case-3 Partition-3 and PartitionBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a000fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It creats folder by column and creates files as per partition, in this case 3 files\n",
    "dataframe2.write.format(\"csv\").mode('overwrite').option(\"path\",output_path).partitionBy(\"area_type\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7aaa9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # It creates a table\n",
    "dataframe2.select(\"*\").write.format(\"jdbc\").options(**db_cred).\\\n",
    "option(\"dbtable\", 'house102').mode(\"overwrite\").partitionBy(\"area_type\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49886b78",
   "metadata": {},
   "source": [
    "### Case-4 Partition-3 and bucketBy\n",
    "Bucketing is an optimization method that breaks down data into more manageable parts (buckets) to determine the data partitioning while it is written out. The motivation for this method is to make successive reads of the data more performant for downstream jobs if the SQL operators can make use of this property.\n",
    "\n",
    "here bucketBy distributes data to a fixed number of buckets (16 in our case) and can be used when the number of unique values is not limited. If the number of unique values is limited, it's better to use a partitioning instead of a bucketing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "97913a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: Save does not support bucketBy right now.\n",
    "#2: It creats\n",
    "dataframe2.write.mode('overwrite').option(\"path\",output_path).bucketBy(16,\"area_type\").\\\n",
    "saveAsTable('bucketed', format='parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "98427ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # It creates a table\n",
    "dataframe2.select(\"*\").write.format(\"jdbc\").options(**db_cred).\\\n",
    "option(\"dbtable\", 'bucketed').mode(\"overwrite\").bucketBy(16,\"area_type\").saveAsTable(\"bucketed101\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
