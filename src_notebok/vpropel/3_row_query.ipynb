{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2035f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as FS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06290ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"Spark-Query\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "103787a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.55:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark-Query</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1c5c717a020>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afa57a9",
   "metadata": {},
   "source": [
    "**1. Given 2 dataframe having different number of columns structure how will you merge it on row basis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e4cde06",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1= [(1,\"manish\",\"data engineer\"),(2,\"rani\",\"data analyst\"),(3,\"manju\",\"data science\")]\n",
    "column1 =[\"id\",\"name\",\"department\"]\n",
    "\n",
    "data2= [(3,\"harish\"),(4,\"monish\"),(5,\"priti\")]\n",
    "column2 =[\"id\",\"name\"]\n",
    "dataframe1 = spark.createDataFrame(data = data1,schema =column1 )\n",
    "dataframe2 = spark.createDataFrame(data = data2,schema =column2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beecf689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+\n",
      "| id|  name|   department|\n",
      "+---+------+-------------+\n",
      "|  1|manish|data engineer|\n",
      "|  2|  rani| data analyst|\n",
      "|  3| manju| data science|\n",
      "+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "501240d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  3|harish|\n",
      "|  4|monish|\n",
      "|  5| priti|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce310b",
   "metadata": {},
   "source": [
    "**Apply Union to merge data on row basis. but, union operation require no: of columns should be same.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "884d6507",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe2 = dataframe2.withColumn(\"department\",FS.lit('null'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ec49ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "| id|  name|department|\n",
      "+---+------+----------+\n",
      "|  3|harish|      null|\n",
      "|  4|monish|      null|\n",
      "|  5| priti|      null|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "790ea28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answ1 = dataframe1.union(dataframe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3009fae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+\n",
      "| id|  name|   department|\n",
      "+---+------+-------------+\n",
      "|  1|manish|data engineer|\n",
      "|  2|  rani| data analyst|\n",
      "|  3| manju| data science|\n",
      "|  3|harish|         null|\n",
      "|  4|monish|         null|\n",
      "|  5| priti|         null|\n",
      "+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answ1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d774f01",
   "metadata": {},
   "source": [
    "**2. how to convert any dataframe into sql views/tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ecc4249",
   "metadata": {},
   "outputs": [],
   "source": [
    "answ1.createOrReplaceTempView(\"ans_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52e04eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+\n",
      "| id|  name|   department|\n",
      "+---+------+-------------+\n",
      "|  1|manish|data engineer|\n",
      "|  2|  rani| data analyst|\n",
      "|  3| manju| data science|\n",
      "|  3|harish|         null|\n",
      "|  4|monish|         null|\n",
      "|  5| priti|         null|\n",
      "+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from ans_view\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316162c",
   "metadata": {},
   "source": [
    "**3. how to convert comma separated values into sql columns<br>**\n",
    "d = (\"122334\",\"2221\",324251\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ab19846",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [{'name':\"khan\",\"id\":\"mango,banana,orange\"}]\n",
    "df1 = spark.createDataFrame(data = d )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3703c59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+\n",
      "|                 id|name|\n",
      "+-------------------+----+\n",
      "|mango,banana,orange|khan|\n",
      "+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1026691a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|                  id|name|\n",
      "+--------------------+----+\n",
      "|[mango, banana, o...|khan|\n",
      "+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df1 = df1.withColumn(\"id\", FS.explode(FS.array(df1.id)))\n",
    "# Note: explode not working here, will check later\n",
    "df1 = df1.withColumn(\"id\",FS.split(\"id\",\",\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2bf21c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|name|   col|\n",
      "+----+------+\n",
      "|khan| mango|\n",
      "|khan|banana|\n",
      "|khan|orange|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df1.select(\"name\",FS.explode(\"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26fb919",
   "metadata": {},
   "source": [
    "**3. How to convert multiple rows into single rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19399e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| name|  language|\n",
      "+-----+----------+\n",
      "|James|      Java|\n",
      "|James|    Python|\n",
      "|James|    Python|\n",
      "| Anna|       PHP|\n",
      "| Anna|Javascript|\n",
      "|Maria|      Java|\n",
      "|Maria|       C++|\n",
      "|James|     Scala|\n",
      "| Anna|       PHP|\n",
      "| Anna|      HTML|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lang_data = [('James','Java'),\n",
    "  ('James','Python'),\n",
    "  ('James','Python'),\n",
    "  ('Anna','PHP'),\n",
    "  ('Anna','Javascript'),\n",
    "  ('Maria','Java'),\n",
    "  ('Maria','C++'),\n",
    "  ('James','Scala'),\n",
    "  ('Anna','PHP'),\n",
    "  ('Anna','HTML')\n",
    "]\n",
    "column = ['name','language']\n",
    "datafrm = spark.createDataFrame(data = lang_data, schema = column)\n",
    "datafrm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "125f80bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "| name|                lang|\n",
      "+-----+--------------------+\n",
      "|James|[Scala, Java, Pyt...|\n",
      "| Anna|[PHP, Javascript,...|\n",
      "|Maria|         [Java, C++]|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datafrm.groupBy(\"name\").agg(FS.collect_set(\"language\").alias(\"lang\")).show()\n",
    "# datafrm.groupBy('name').agg(FS.collect_list(\"language\").alias(\"prog\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f50589",
   "metadata": {},
   "source": [
    "**4. Format the state_dataframe to below format**\n",
    "```\n",
    "+---------+\n",
    "|city_name|\n",
    "+---------+\n",
    "|      Goa|\n",
    "|       AP|\n",
    "|      Blr|\n",
    "+---------+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3ec913b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|city1|city2|city3|\n",
      "+-----+-----+-----+\n",
      "|  Goa| NULL| NULL|\n",
      "| NULL|   AP| NULL|\n",
      "| NULL| NULL|  Blr|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_data = [('Goa',None,None),(None,'AP',None),(None,None,'Blr')]\n",
    "schema = \"city1 string,city2 string,city3 string\"\n",
    "state_df = spark.createDataFrame(data =state_data,schema =  schema)\n",
    "state_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1f77fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|city_name|\n",
      "+---------+\n",
      "|      Goa|\n",
      "|       AP|\n",
      "|      Blr|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_df.withColumn(\"city_name\",FS.coalesce(state_df.city1,state_df.city2,state_df.city3)).\\\n",
    "drop(\"city1\").drop(\"city2\").drop(\"city3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75307c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
