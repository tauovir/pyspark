{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "674e7ba0",
   "metadata": {},
   "source": [
    "**The below question were asked during the interview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da4e87f",
   "metadata": {},
   "source": [
    "### Python Problem2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2149e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 'y', '1': '2', 't': 'y'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Problem-1\n",
    "data_dict = [\n",
    "{ \"x\": \"y\"},\n",
    "{ \"1\": \"2\"},\n",
    "{ \"t\": \"y\"}\n",
    "]\n",
    "\n",
    "# Output: {\"x\": \"y\", \"1\": \"2\", \"t\": \"y\"}\n",
    "\n",
    "def reformat(data_dict):\n",
    "    response = {}\n",
    "    for row in data_dict:\n",
    "        for key,val in row.items():\n",
    "            response[key] = val\n",
    "    return response\n",
    "print(reformat(data_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b51bdca",
   "metadata": {},
   "source": [
    "### Python Problem2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de1e9f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c']\n",
      "{'c'}\n"
     ]
    }
   ],
   "source": [
    "x = [ 'a', 'b', 'c' ] \n",
    "y = [ 'a', 'b' ]\n",
    "# how to get x - y operation\n",
    "res1 = [ele for ele in x if ele not in y]\n",
    "print(res1)\n",
    "res2 = set(x) - set(y)\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cabaea3",
   "metadata": {},
   "source": [
    "### Pyspark Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b26ec9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pyspark:\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as FS\n",
    "from pyspark.sql import Window as WN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9acd2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    {\"id\":\"abc\",\"amount\":10},\n",
    "    {\"id\":\"def\",\"amount\":20}\n",
    "]\n",
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"US_Client\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac34e3bf",
   "metadata": {},
   "source": [
    "## Problem-1\n",
    "```\n",
    "+------+---+\n",
    "|amount| id|\n",
    "+------+---+\n",
    "|    10|abc|\n",
    "|    20|def|\n",
    "+------+---+\n",
    "```\n",
    "**Output**\n",
    "```\n",
    "['{\"amount\":10,\"id\":\"abc\"}', '{\"amount\":20,\"id\":\"def\"}']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c75b7624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|amount| id|\n",
      "+------+---+\n",
      "|    10|abc|\n",
      "|    20|def|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(dataset)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289b941d",
   "metadata": {},
   "source": [
    "**output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf28b3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"amount\":10,\"id\":\"abc\"}', '{\"amount\":20,\"id\":\"def\"}']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toJSON().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17af97d1",
   "metadata": {},
   "source": [
    "### Problem2\n",
    "Convert dataframe to below format\n",
    "~~~\n",
    "Output JSON 2:\n",
    "{\n",
    "\"ID\" : [ \"abc\", \"def\"],\n",
    "\"amount\" : [ 10, 20]\n",
    "}\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "803f6ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to dictionary\n",
    "dict_data = df.groupBy().agg(FS.collect_list(\"id\").alias(\"id\"), FS.collect_list(\"amount\").alias(\"amount\")).collect()[0].asDict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58a6c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = df.groupBy().agg(FS.collect_list(\"id\").alias(\"id\"),FS.collect_list(\"amount\").alias(\"amount\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b60a53d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': ['abc', 'def'], 'amount': [10, 20]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.collect()[0].asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07e5516f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+---------+-------------------------+\n",
      "|Author|BlogEntries|Caller|Name     |Url                      |\n",
      "+------+-----------+------+---------+-------------------------+\n",
      "|jangcy|100        |jangcy|something|https://stackoverflow.com|\n",
      "+------+-----------+------+---------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newJson = '{\"Name\":\"something\",\"Url\":\"https://stackoverflow.com\",\"Author\":\"jangcy\",\"BlogEntries\":100,\"Caller\":\"jangcy\"}'\n",
    "df = spark.read.json(spark.sparkContext.parallelize([newJson]))\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe9947a",
   "metadata": {},
   "source": [
    "### Problem2\n",
    "Convert dataframe to below format\n",
    "~~~\n",
    "A table with following columns\n",
    "Month\t\t\tSales\n",
    "Apr-2020\t\t10,000\n",
    "May-2020\t\t12,000\n",
    "Jun-2020\t\t11,400\n",
    "~~~\n",
    "**Output**:\n",
    "~~~\n",
    "Month\t\t\tSales\t\tGrowth%\n",
    "Apr-2020\t\t10,000\t\tNull\n",
    "May-2020\t\t12,000\t\t20\n",
    "Jun-2020\t\t11,400\t\t-5\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "988d937e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|   month|sales|\n",
      "+--------+-----+\n",
      "|Apr-2020|10000|\n",
      "|May-2020|12000|\n",
      "|Jun-2020|11400|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = [\n",
    "    {\"month\":\"Apr-2020\",\"sales\":10000},\n",
    "     {\"month\":\"May-2020\",\"sales\":12000},\n",
    "     {\"month\":\"Jun-2020\",\"sales\":11400},\n",
    "]\n",
    "sale_df = spark.createDataFrame(dataset)\n",
    "sale_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bd168fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import DateType\n",
    "def convert_date(col):\n",
    "    from datetime import datetime\n",
    "    part = col.split(\"-\")\n",
    "    date_str = part[-1] + \"-\" +  part[0] + \"-01\"\n",
    "    date = datetime.strptime(date_str, '%Y-%b-%d')\n",
    "    return date\n",
    "\n",
    "# Converting function to UDF \n",
    "convertUDF = FS.udf(lambda z: convert_date(z),DateType())\n",
    "# sale_df.withColumn(\"month1\",FS.to_date('month1','m-Y')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e3061bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_sf2 = sale_df.select(\"sales\",convertUDF(FS.col('month')).alias(\"month\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a30b1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "039ed972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eb88c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_sf2.createOrReplaceTempView(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1315d1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|sales|     month|\n",
      "+-----+----------+\n",
      "|10000|2020-04-01|\n",
      "|12000|2020-05-01|\n",
      "|11400|2020-06-01|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "21f896ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+\n",
      "|     month|sales|growth|\n",
      "+----------+-----+------+\n",
      "|2020-04-01|10000|  NULL|\n",
      "|2020-05-01|12000|  20.0|\n",
      "|2020-06-01|11400|  -5.0|\n",
      "+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "with SalesData as (\n",
    "select month,sales,\n",
    "lag(sales) over(order by month) as previous_sale from sales\n",
    ")\n",
    "select month,sales,\n",
    "CASE\n",
    "WHEN previous_sale IS NULL THEN NULL\n",
    "ELSE ROUND(((sales - previous_sale)/previous_sale * 100),2)\n",
    "END AS growth\n",
    "from SalesData;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "552d8354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------------+------+\n",
      "|     month|sales|previous_sale|growth|\n",
      "+----------+-----+-------------+------+\n",
      "|2020-04-01|10000|         NULL|  NULL|\n",
      "|2020-05-01|12000|        10000|  20.0|\n",
      "|2020-06-01|11400|        12000|  -5.0|\n",
      "+----------+-----+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Similarly with Pyspakr Dataframe\n",
    "sales_sf2.withColumn(\"previous_sale\",FS.lag('sales').over(WN.partitionBy().orderBy(FS.col('month').asc()))).\\\n",
    "select(\"month\",\"sales\",FS.col('previous_sale'),FS.when(FS.col('previous_sale') == 'null','null').\\\n",
    "       otherwise((FS.col('sales')- FS.col('previous_sale'))/FS.col('previous_sale') * 100).alias(\"growth\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b290949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "32feaae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|dno|                 eno|\n",
      "+---+--------------------+\n",
      "|  1|  [1, 2, 3, 4, 5, 6]|\n",
      "|  2|[7, 8, 9, 10, 11,...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, [1,2,3,4,5,6]), (2, [7,8,9,10,11,12,13])]\n",
    "df_arr = spark.createDataFrame(data, [\"dno\", \"eno\"])\n",
    "df_arr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1a94564d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|dno|eno|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  1|  2|\n",
      "|  1|  3|\n",
      "|  1|  4|\n",
      "|  1|  5|\n",
      "|  1|  6|\n",
      "|  2|  7|\n",
      "|  2|  8|\n",
      "|  2|  9|\n",
      "|  2| 10|\n",
      "|  2| 11|\n",
      "|  2| 12|\n",
      "|  2| 13|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_exploded = df_arr.withColumn(\"eno\", FS.explode(FS.col(\"eno\")))\n",
    "df_exploded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "60256b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "|dno|eno|group|\n",
      "+---+---+-----+\n",
      "|  1|  1|    0|\n",
      "|  1|  2|    1|\n",
      "|  1|  3|    2|\n",
      "|  1|  4|    3|\n",
      "|  1|  5|    0|\n",
      "|  1|  6|    1|\n",
      "|  2|  7|    2|\n",
      "|  2|  8|    3|\n",
      "|  2|  9|    0|\n",
      "|  2| 10|    1|\n",
      "|  2| 11|    2|\n",
      "|  2| 12|    3|\n",
      "|  2| 13|    0|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_exploded = df_exploded.withColumn(\"group\", (col(\"eno\") - 1) % 4)\n",
    "df_exploded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4120d659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|dno|eno    |\n",
      "+---+-------+\n",
      "|1  |[4]    |\n",
      "|1  |[1, 5] |\n",
      "|1  |[2, 6] |\n",
      "|1  |[3]    |\n",
      "|2  |[8, 12]|\n",
      "|2  |[7, 11]|\n",
      "|2  |[9, 13]|\n",
      "|2  |[10]   |\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed = df_exploded.groupby(\"dno\", \"group\").agg(FS.collect_list(\"eno\").alias(\"eno\"))\n",
    "\n",
    "# Drop the 'group' column\n",
    "df_transformed = df_transformed.drop(\"group\")\n",
    "\n",
    "# Show the transformed dataframe\n",
    "df_transformed.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
